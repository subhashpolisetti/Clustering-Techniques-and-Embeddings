{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhashpolisetti/Clustering-Techniques-and-Embeddings/blob/main/7_Document_Clustering_with_Sentence_Transformers_and_KMeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering Documents with Sentence Transformers and KMeans\n",
        "\n",
        "This notebook demonstrates how to perform document clustering using **Sentence Transformers** for generating embeddings and **KMeans** for clustering. We will use a set of example documents to show how the embeddings are generated, and then apply clustering techniques to categorize the documents into groups.\n",
        "\n",
        "### Steps in the Notebook:\n",
        "1. **Generate Document Embeddings**:\n",
        "   - We use the **SentenceTransformer** model (`all-MiniLM-L6-v2`) to generate embeddings for a collection of documents.\n",
        "   - These embeddings are vector representations of the documents in a high-dimensional space, capturing the semantic meaning of each document.\n",
        "\n",
        "2. **Clustering with KMeans**:\n",
        "   - The embeddings are then clustered using the **KMeans** algorithm from scikit-learn. This algorithm partitions the documents into clusters based on the similarity of their embeddings.\n",
        "   - The number of clusters is specified (`num_clusters = 3` in this case), but it can be adjusted as needed.\n",
        "\n",
        "3. **Displaying Clustering Results**:\n",
        "   - For each document, we display the assigned cluster and a preview of the content.\n",
        "   - This helps in understanding which documents are grouped together and the semantic similarities between them.\n",
        "\n",
        "4. **Optional: Cluster Summarization**:\n",
        "   - After clustering, we can generate summaries for each cluster to provide a high-level overview of the documents in each group.\n",
        "   - This feature can be enhanced by using a language model to generate custom summaries for each cluster.\n",
        "\n",
        "### Example Documents:\n",
        "- \"Machine learning is a subset of artificial intelligence.\"\n",
        "- \"Natural language processing deals with the interaction between computers and human language.\"\n",
        "- \"Deep learning uses neural networks with multiple layers.\"\n",
        "- \"Reinforcement learning is learning what to do to maximize a reward.\"\n",
        "- \"Computer vision is the field of AI that trains computers to interpret visual information.\"\n",
        "\n",
        "### Expected Output:\n",
        "- The notebook will output the cluster assignments for each document, along with the content.\n",
        "- You can also optionally generate summaries for each cluster, which can help in understanding the themes of each group.\n",
        "\n",
        "This notebook provides a simple way to cluster text documents using embeddings and clustering techniques and can be extended for various natural language processing tasks such as topic modeling, document retrieval, and summarization.\n"
      ],
      "metadata": {
        "id": "q6UoaLcdE0-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zd71Foc4xWV",
        "outputId": "b539d0eb-b6a3-4908-8d79-3105b293988a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llm\n",
            "  Downloading llm-0.18-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from llm) (8.1.7)\n",
            "Requirement already satisfied: openai>=1.0 in /usr/local/lib/python3.10/dist-packages (from llm) (1.54.4)\n",
            "Collecting click-default-group>=1.2.3 (from llm)\n",
            "  Downloading click_default_group-1.2.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sqlite-utils>=3.37 (from llm)\n",
            "  Downloading sqlite_utils-3.37-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting sqlite-migrate>=0.1a2 (from llm)\n",
            "  Downloading sqlite_migrate-0.1b0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from llm) (2.9.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llm) (6.0.2)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.10/dist-packages (from llm) (1.5.0)\n",
            "Collecting python-ulid (from llm)\n",
            "  Downloading python_ulid-3.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from llm) (75.1.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from llm) (24.1.2)\n",
            "Collecting puremagic (from llm)\n",
            "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->llm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.2->llm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.2->llm) (2.23.4)\n",
            "Collecting sqlite-fts4 (from sqlite-utils>=3.37->llm)\n",
            "  Downloading sqlite_fts4-1.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sqlite-utils>=3.37->llm) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from sqlite-utils>=3.37->llm) (2.8.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->llm) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->llm) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->llm) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->llm) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->llm) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->sqlite-utils>=3.37->llm) (1.16.0)\n",
            "Downloading llm-0.18-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_default_group-1.2.4-py2.py3-none-any.whl (4.1 kB)\n",
            "Downloading sqlite_migrate-0.1b0-py3-none-any.whl (10.0 kB)\n",
            "Downloading sqlite_utils-3.37-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_ulid-3.0.0-py3-none-any.whl (11 kB)\n",
            "Downloading sqlite_fts4-1.0.3-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: sqlite-fts4, puremagic, python-ulid, click-default-group, sqlite-utils, sqlite-migrate, llm\n",
            "Successfully installed click-default-group-1.2.4 llm-0.18 puremagic-1.28 python-ulid-3.0.0 sqlite-fts4-1.0.3 sqlite-migrate-0.1b0 sqlite-utils-3.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcgfVbzi4sLW"
      },
      "outputs": [],
      "source": [
        "import click\n",
        "import json\n",
        "import llm\n",
        "import numpy as np\n",
        "import sklearn.cluster\n",
        "import sqlite_utils\n",
        "import textwrap\n",
        "\n",
        "DEFAULT_SUMMARY_PROMPT = \"\"\"\n",
        "Short, concise title for this cluster of related documents.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "@llm.hookimpl\n",
        "def register_commands(cli):\n",
        "    @cli.command()\n",
        "    @click.argument(\"collection\")\n",
        "    @click.argument(\"n\", type=int)\n",
        "    @click.option(\n",
        "        \"--truncate\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Truncate content to this many characters - 0 for no truncation\",\n",
        "    )\n",
        "    @click.option(\n",
        "        \"-d\",\n",
        "        \"--database\",\n",
        "        type=click.Path(\n",
        "            file_okay=True, allow_dash=False, dir_okay=False, writable=True\n",
        "        ),\n",
        "        envvar=\"LLM_EMBEDDINGS_DB\",\n",
        "        help=\"SQLite database file containing embeddings\",\n",
        "    )\n",
        "    @click.option(\n",
        "        \"--summary\", is_flag=True, help=\"Generate summary title for each cluster\"\n",
        "    )\n",
        "    @click.option(\"-m\", \"--model\", help=\"LLM model to use for the summary\")\n",
        "    @click.option(\"--prompt\", help=\"Custom prompt to use for the summary\")\n",
        "    def cluster(collection, n, truncate, database, summary, model, prompt):\n",
        "        \"\"\"\n",
        "        Generate clusters from embeddings in a collection\n",
        "\n",
        "        Example usage, to create 10 clusters:\n",
        "\n",
        "        \\b\n",
        "            llm cluster my_collection 10\n",
        "\n",
        "        Outputs a JSON array of {\"id\": \"cluster_id\", \"items\": [list of items]}\n",
        "\n",
        "        Pass --summary to generate a summary for each cluster, using the default\n",
        "        language model or the model you specify with --model.\n",
        "        \"\"\"\n",
        "        from llm.cli import get_default_model, get_key\n",
        "\n",
        "        clustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=n, n_init=\"auto\")\n",
        "        if database:\n",
        "            db = sqlite_utils.Database(database)\n",
        "        else:\n",
        "            db = sqlite_utils.Database(llm.user_dir() / \"embeddings.db\")\n",
        "        rows = [\n",
        "            (row[0], llm.decode(row[1]), row[2])\n",
        "            for row in db.execute(\n",
        "                \"\"\"\n",
        "            select id, embedding, content from embeddings\n",
        "            where collection_id = (\n",
        "                select id from collections where name = ?\n",
        "            )\n",
        "        \"\"\",\n",
        "                [collection],\n",
        "            ).fetchall()\n",
        "        ]\n",
        "        to_cluster = np.array([item[1] for item in rows])\n",
        "        clustering_model.fit(to_cluster)\n",
        "        assignments = clustering_model.labels_\n",
        "\n",
        "        def truncate_text(text):\n",
        "            if not text:\n",
        "                return None\n",
        "            if truncate > 0:\n",
        "                return text[:truncate]\n",
        "            else:\n",
        "                return text\n",
        "\n",
        "        # Each one corresponds to an ID\n",
        "        clusters = {}\n",
        "        for (id, _, content), cluster in zip(rows, assignments):\n",
        "            clusters.setdefault(str(cluster), []).append(\n",
        "                {\"id\": str(id), \"content\": truncate_text(content)}\n",
        "            )\n",
        "        # Re-arrange into a list\n",
        "        output_clusters = [{\"id\": k, \"items\": v} for k, v in clusters.items()]\n",
        "\n",
        "        # Do we need to generate summaries?\n",
        "        if summary:\n",
        "            model = llm.get_model(model or get_default_model())\n",
        "            if model.needs_key:\n",
        "                model.key = get_key(\"\", model.needs_key, model.key_env_var)\n",
        "            prompt = prompt or DEFAULT_SUMMARY_PROMPT\n",
        "            click.echo(\"[\")\n",
        "            for cluster, is_last in zip(\n",
        "                output_clusters, [False] * (len(output_clusters) - 1) + [True]\n",
        "            ):\n",
        "                click.echo(\"  {\")\n",
        "                click.echo('    \"id\": {},'.format(json.dumps(cluster[\"id\"])))\n",
        "                click.echo(\n",
        "                    '    \"items\": '\n",
        "                    + textwrap.indent(\n",
        "                        json.dumps(cluster[\"items\"], indent=2), \"    \"\n",
        "                    ).lstrip()\n",
        "                    + \",\"\n",
        "                )\n",
        "                prompt_content = \"\\n\".join(\n",
        "                    [item[\"content\"] for item in cluster[\"items\"] if item[\"content\"]]\n",
        "                )\n",
        "                if prompt_content.strip():\n",
        "                    summary = model.prompt(\n",
        "                        prompt_content,\n",
        "                        system=prompt,\n",
        "                    ).text()\n",
        "                else:\n",
        "                    summary = None\n",
        "                click.echo('    \"summary\": {}'.format(json.dumps(summary)))\n",
        "                click.echo(\"  }\" + (\",\" if not is_last else \"\"))\n",
        "            click.echo(\"]\")\n",
        "        else:\n",
        "            click.echo(json.dumps(output_clusters, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import llm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\n",
        "# Create a collection and embed the documents\n",
        "collection = llm.Collection(\"documents\", model_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = []\n",
        "valid_documents = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    try:\n",
        "        embedding = collection.embed(f\"doc_{i}\", doc, store=True)\n",
        "        if embedding is not None:\n",
        "            embeddings.append(embedding)\n",
        "            valid_documents.append(doc)\n",
        "        else:\n",
        "            print(f\"Warning: Embedding for document {i} is None\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i}: {str(e)}\")\n",
        "\n",
        "# Print the number of embeddings\n",
        "print(f\"Number of embeddings: {len(embeddings)}\")\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "if embeddings:\n",
        "    embeddings_array = np.array(embeddings)\n",
        "    print(f\"Shape of embeddings array: {embeddings_array.shape}\")\n",
        "\n",
        "    # Check for NaN values\n",
        "    nan_count = np.isnan(embeddings_array).sum()\n",
        "    print(f\"Number of NaN values: {nan_count}\")\n",
        "\n",
        "    # Remove any NaN values\n",
        "    embeddings_array = embeddings_array[~np.isnan(embeddings_array).any(axis=1)]\n",
        "    print(f\"Shape after removing NaNs: {embeddings_array.shape}\")\n",
        "\n",
        "    # Check if we have any valid embeddings\n",
        "    if embeddings_array.size > 0:\n",
        "        # Perform K-means clustering\n",
        "        num_clusters = min(3, len(embeddings_array))  # Ensure we don't have more clusters than data points\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
        "\n",
        "        # Print the clustering results\n",
        "        for i, (doc, label) in enumerate(zip(valid_documents, cluster_labels)):\n",
        "            print(f\"Document {i}: Cluster {label}\")\n",
        "            print(f\"Content: {doc}\")\n",
        "            print()\n",
        "\n",
        "        # Use llm-cluster to generate summaries for each cluster\n",
        "        try:\n",
        "            from llm_cluster import cluster_embeddings\n",
        "            cluster_results = cluster_embeddings(collection, num_clusters, summary=True)\n",
        "\n",
        "            # Print the cluster summaries\n",
        "            for cluster in cluster_results:\n",
        "                print(f\"Cluster {cluster['id']}:\")\n",
        "                print(f\"Summary: {cluster['summary']}\")\n",
        "                print(\"Items:\")\n",
        "                for item in cluster['items']:\n",
        "                    print(f\"- {item['content']}\")\n",
        "                print()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while generating cluster summaries: {str(e)}\")\n",
        "    else:\n",
        "        print(\"No valid embeddings after removing NaNs. Cannot perform clustering.\")\n",
        "else:\n",
        "    print(\"No valid embeddings. Cannot perform clustering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AomKbO24tc6",
        "outputId": "8d5d9100-36ef-4545-a60c-adec4a81c6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Embedding for document 0 is None\n",
            "Warning: Embedding for document 1 is None\n",
            "Warning: Embedding for document 2 is None\n",
            "Warning: Embedding for document 3 is None\n",
            "Warning: Embedding for document 4 is None\n",
            "Warning: Embedding for document 5 is None\n",
            "Warning: Embedding for document 6 is None\n",
            "Warning: Embedding for document 7 is None\n",
            "Warning: Embedding for document 8 is None\n",
            "Warning: Embedding for document 9 is None\n",
            "Number of embeddings: 0\n",
            "No valid embeddings. Cannot perform clustering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "cP65e6NI5Wha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Document {i}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R49tUM-8Xe8",
        "outputId": "57aebca0-fe69-48eb-8172-b5fbdfa074d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0: Machine learning is a subset of artificial intelligence.\n",
            "Document 1: Natural language processing deals with the interaction between computers and human language.\n",
            "Document 2: Deep learning uses neural networks with multiple layers.\n",
            "Document 3: Reinforcement learning is learning what to do to maximize a reward.\n",
            "Document 4: Computer vision is the field of AI that trains computers to interpret visual information.\n",
            "Document 5: Clustering is an unsupervised learning technique.\n",
            "Document 6: Classification is a supervised learning task.\n",
            "Document 7: Regression predicts continuous values.\n",
            "Document 8: Neural networks are inspired by the human brain.\n",
            "Document 9: Support vector machines are used for classification and regression tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(documents)\n",
        "print(f\"Shape of embeddings: {embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYEGR5fh8Y8w",
        "outputId": "dbd1e2e1-cdd4-4091-933a-cf3f258aff65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings: (10, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i, doc in enumerate(documents):\n",
        "    try:\n",
        "        embedding = model.encode(doc)\n",
        "        embeddings.append(embedding)\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i}: {str(e)}\")\n",
        "\n",
        "embeddings_array = np.array(embeddings)"
      ],
      "metadata": {
        "id": "a8QCP1rL8at9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "all_embeddings = []\n",
        "for i in range(0, len(documents), batch_size):\n",
        "    batch = documents[i:i+batch_size]\n",
        "    batch_embeddings = model.encode(batch)\n",
        "    all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "embeddings_array = np.array(all_embeddings)"
      ],
      "metadata": {
        "id": "jW4_vrv78crs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import csv\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Read the CSV file and extract the text to be embedded\n",
        "# Create a sample dataset of documents\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Natural language processing deals with the interaction between computers and human language.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "    \"Reinforcement learning is learning what to do to maximize a reward.\",\n",
        "    \"Computer vision is the field of AI that trains computers to interpret visual information.\",\n",
        "    \"Clustering is an unsupervised learning technique.\",\n",
        "    \"Classification is a supervised learning task.\",\n",
        "    \"Regression predicts continuous values.\",\n",
        "    \"Neural networks are inspired by the human brain.\",\n",
        "    \"Support vector machines are used for classification and regression tasks.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(documents)\n",
        "\n",
        "# Perform K-means clustering\n",
        "num_clusters = 3  # Adjust as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Print the clustering results\n",
        "for i, (doc, label) in enumerate(zip(documents, cluster_labels)):\n",
        "    print(f\"Document {i}: Cluster {label}\")\n",
        "    print(f\"Content: {doc[:100]}...\")  # Print first 100 characters\n",
        "    print()\n",
        "\n",
        "# Optional: Generate summaries for each cluster\n",
        "# This part depends on how you want to summarize the clusters\n",
        "# You might need to implement a custom summarization method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5p_LIJ28fi8",
        "outputId": "15b863b2-aba6-4d8c-d0d5-62e84d1981aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0: Cluster 2\n",
            "Content: Machine learning is a subset of artificial intelligence....\n",
            "\n",
            "Document 1: Cluster 1\n",
            "Content: Natural language processing deals with the interaction between computers and human language....\n",
            "\n",
            "Document 2: Cluster 2\n",
            "Content: Deep learning uses neural networks with multiple layers....\n",
            "\n",
            "Document 3: Cluster 0\n",
            "Content: Reinforcement learning is learning what to do to maximize a reward....\n",
            "\n",
            "Document 4: Cluster 2\n",
            "Content: Computer vision is the field of AI that trains computers to interpret visual information....\n",
            "\n",
            "Document 5: Cluster 1\n",
            "Content: Clustering is an unsupervised learning technique....\n",
            "\n",
            "Document 6: Cluster 1\n",
            "Content: Classification is a supervised learning task....\n",
            "\n",
            "Document 7: Cluster 1\n",
            "Content: Regression predicts continuous values....\n",
            "\n",
            "Document 8: Cluster 2\n",
            "Content: Neural networks are inspired by the human brain....\n",
            "\n",
            "Document 9: Cluster 1\n",
            "Content: Support vector machines are used for classification and regression tasks....\n",
            "\n"
          ]
        }
      ]
    }
  ]
}